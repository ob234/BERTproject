{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "648032caa94f4d5bb2593c892bd54754": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_665d321a9cc44dbdaa6058624772403d",
              "IPY_MODEL_2b11f21ffd9244778c2d85badb1aeaec",
              "IPY_MODEL_44a0cf64eeb844c3912433f764831510"
            ],
            "layout": "IPY_MODEL_08e90dbc7d5c474c919a75b839869b8a"
          }
        },
        "665d321a9cc44dbdaa6058624772403d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b6e58de2ff1445383d949df3013cc72",
            "placeholder": "​",
            "style": "IPY_MODEL_6371aa71d1e147ce8ca32d225e3a69d4",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "2b11f21ffd9244778c2d85badb1aeaec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9a6a5b30d1b4687a166507a116de50e",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6f8d8fcb1824a369bebdbe13bae8c6b",
            "value": 231508
          }
        },
        "44a0cf64eeb844c3912433f764831510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3174475d64e04bb2be7ece342f9cc563",
            "placeholder": "​",
            "style": "IPY_MODEL_085f8f4894414e949e554a4629ebd66b",
            "value": " 232k/232k [00:00&lt;00:00, 12.1MB/s]"
          }
        },
        "08e90dbc7d5c474c919a75b839869b8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b6e58de2ff1445383d949df3013cc72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6371aa71d1e147ce8ca32d225e3a69d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9a6a5b30d1b4687a166507a116de50e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6f8d8fcb1824a369bebdbe13bae8c6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3174475d64e04bb2be7ece342f9cc563": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "085f8f4894414e949e554a4629ebd66b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3e0d7d1e7624c8cb6c6daa14138c43c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ee969321515472ea87b692f734610d7",
              "IPY_MODEL_4d2d44b01428487fbf4e491f9836867a",
              "IPY_MODEL_794f8b9ecb164e97baf8b028aa3fcee8"
            ],
            "layout": "IPY_MODEL_c36ef857b65042b6a13fd54d0709cc47"
          }
        },
        "6ee969321515472ea87b692f734610d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e42885910234ebc961f28ba7f4f6cfe",
            "placeholder": "​",
            "style": "IPY_MODEL_fca1c7cde4d94abb8276bf7c57353dab",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "4d2d44b01428487fbf4e491f9836867a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fec2919029ce4df5b73a9014bf5b0950",
            "max": 286,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_090017e9270f4ed684932232c82e6f5b",
            "value": 286
          }
        },
        "794f8b9ecb164e97baf8b028aa3fcee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32df66e5be8843c4a33c24010abb3fd2",
            "placeholder": "​",
            "style": "IPY_MODEL_7d941f80e6b84758ac41d2835ee8b79f",
            "value": " 286/286 [00:00&lt;00:00, 23.6kB/s]"
          }
        },
        "c36ef857b65042b6a13fd54d0709cc47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e42885910234ebc961f28ba7f4f6cfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fca1c7cde4d94abb8276bf7c57353dab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fec2919029ce4df5b73a9014bf5b0950": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "090017e9270f4ed684932232c82e6f5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32df66e5be8843c4a33c24010abb3fd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d941f80e6b84758ac41d2835ee8b79f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16bd5ed102ab4a199fdf62957fe562db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d6d95e6d24d480e9bbf09a96e70c1fc",
              "IPY_MODEL_d79a10e6eebc48b99d2748913a6d9b6b",
              "IPY_MODEL_49474748743047248350fa27eb145f91"
            ],
            "layout": "IPY_MODEL_01a5fff3fea74c448f0ee838e7d1a652"
          }
        },
        "5d6d95e6d24d480e9bbf09a96e70c1fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe10fa7cbb604a4384ee9e16ade348f8",
            "placeholder": "​",
            "style": "IPY_MODEL_7234bcd5375245e08b68d8d42ea86f1f",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "d79a10e6eebc48b99d2748913a6d9b6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a968a8e6e97140f4836d8699cf848535",
            "max": 45106985,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c3c8bf6d0774882bddd7773fad63be2",
            "value": 45106985
          }
        },
        "49474748743047248350fa27eb145f91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b0fd797bbf64067b32dfccac2e60f76",
            "placeholder": "​",
            "style": "IPY_MODEL_d4b23997ee294341a2e241654614d4a7",
            "value": " 45.1M/45.1M [00:00&lt;00:00, 419MB/s]"
          }
        },
        "01a5fff3fea74c448f0ee838e7d1a652": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe10fa7cbb604a4384ee9e16ade348f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7234bcd5375245e08b68d8d42ea86f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a968a8e6e97140f4836d8699cf848535": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c3c8bf6d0774882bddd7773fad63be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b0fd797bbf64067b32dfccac2e60f76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b23997ee294341a2e241654614d4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers > /dev/null"
      ],
      "metadata": {
        "id": "_hlhzENkP2lg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "3rW_FPHFCNyh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lvm6Wuam8IvO",
        "outputId": "74a7d0ae-c47e-4754-9968-c779c0779a13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "class SentDataset(Dataset):\n",
        "    def __init__(self, fname, word2ix=None, df=None):\n",
        "        if df is None:\n",
        "            self.sents = pd.read_csv(fname)\n",
        "        else:\n",
        "            self.sents = df.dropna(subset=['sentence'])\n",
        "        # reset index necessary for getting context sentences when using unlabeled data\n",
        "        #self.sents = self.sents.dropna(subset=['sentence'])\n",
        "        if word2ix is not None:\n",
        "            self.word2ix = word2ix\n",
        "            self.ix2word = {ix:word for word,ix in self.word2ix.items()}\n",
        "        else:\n",
        "            self.word2ix = {'<PAD>': 0}\n",
        "            self.ix2word = {}\n",
        "            self.min_cnt = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.sents.iloc[idx]\n",
        "        if row.sentence.startswith('[') and row.sentence.endswith(']'):\n",
        "            try:\n",
        "                sentence = eval(row.sentence)\n",
        "            except:\n",
        "                sentence = word_tokenize(row.sentence)\n",
        "        else:\n",
        "            assert isinstance(row.sentence, str)\n",
        "            sentence = word_tokenize(row.sentence)\n",
        "        sent = [x.lower() for x in sentence]\n",
        "        if 'labels' in row:\n",
        "          labels = eval(row.labels)\n",
        "        else:\n",
        "          labels = []\n",
        "        doc_id = row.doc_id\n",
        "        sent_ix = None\n",
        "        if 'sent_index' in row:\n",
        "            sent_ix = row.sent_index\n",
        "        elif 'sent_ix' in row:\n",
        "            sent_ix = row.sent_ix\n",
        "        return sent, labels, doc_id, sent_ix"
      ],
      "metadata": {
        "id": "YOyAoR_01S0b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "import ast\n",
        "def str_to_list(s):\n",
        "    # Use the ast library to safely evaluate the string as a list\n",
        "    return ast.literal_eval(s)\n",
        "\n",
        "sents = pd.read_csv(\"/content/drive/My Drive/Deep Learning Project/Dataset/clip-a-dataset-for-extracting-action-items-for-physicians-from-hospital-discharge-notes-1.0.0/sentence_level.csv\")\n",
        "\n",
        "# Apply the function to the column to convert the strings to lists\n",
        "#sents['sentence'] = sents['sentence'].apply(str_to_list)\n",
        "\n",
        "# Apply the function to the column to convert the strings to lists\n",
        "#sents['labels'] = sents['labels'].apply(str_to_list)\n",
        "train_dataset = pd.read_csv(\"/content/drive/My Drive/Deep Learning Project/Dataset/clip-a-dataset-for-extracting-action-items-for-physicians-from-hospital-discharge-notes-1.0.0/train_ids.csv\", header=None)\n",
        "test_dataset = pd.read_csv(\"/content/drive/My Drive/Deep Learning Project/Dataset/clip-a-dataset-for-extracting-action-items-for-physicians-from-hospital-discharge-notes-1.0.0/test_ids.csv\", header=None)\n",
        "eval_dataset = pd.read_csv(\"/content/drive/My Drive/Deep Learning Project/Dataset/clip-a-dataset-for-extracting-action-items-for-physicians-from-hospital-discharge-notes-1.0.0/val_ids.csv\", header=None)"
      ],
      "metadata": {
        "id": "4svg-BgcjdzE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.columns = ['doc_id']\n",
        "train_dataset = train_dataset.merge(sents, on = \"doc_id\")\n",
        "test_dataset.columns = ['doc_id']\n",
        "test_dataset = test_dataset.merge(sents, on = \"doc_id\")\n",
        "eval_dataset.columns = ['doc_id']\n",
        "eval_dataset = eval_dataset.merge(sents, on = \"doc_id\")"
      ],
      "metadata": {
        "id": "upiBkT1P1T6u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SentDataset('contet',df =train_dataset)\n",
        "eval_dataset = SentDataset('content',df=eval_dataset)\n",
        "test_dataset = SentDataset('content',df=test_dataset)"
      ],
      "metadata": {
        "id": "MJnrJn5-_u4_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1 = BertTokenizer.from_pretrained('prajjwal1/bert-mini')"
      ],
      "metadata": {
        "id": "Z06uyBeyLQ7D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "648032caa94f4d5bb2593c892bd54754",
            "665d321a9cc44dbdaa6058624772403d",
            "2b11f21ffd9244778c2d85badb1aeaec",
            "44a0cf64eeb844c3912433f764831510",
            "08e90dbc7d5c474c919a75b839869b8a",
            "3b6e58de2ff1445383d949df3013cc72",
            "6371aa71d1e147ce8ca32d225e3a69d4",
            "b9a6a5b30d1b4687a166507a116de50e",
            "d6f8d8fcb1824a369bebdbe13bae8c6b",
            "3174475d64e04bb2be7ece342f9cc563",
            "085f8f4894414e949e554a4629ebd66b",
            "e3e0d7d1e7624c8cb6c6daa14138c43c",
            "6ee969321515472ea87b692f734610d7",
            "4d2d44b01428487fbf4e491f9836867a",
            "794f8b9ecb164e97baf8b028aa3fcee8",
            "c36ef857b65042b6a13fd54d0709cc47",
            "6e42885910234ebc961f28ba7f4f6cfe",
            "fca1c7cde4d94abb8276bf7c57353dab",
            "fec2919029ce4df5b73a9014bf5b0950",
            "090017e9270f4ed684932232c82e6f5b",
            "32df66e5be8843c4a33c24010abb3fd2",
            "7d941f80e6b84758ac41d2835ee8b79f"
          ]
        },
        "outputId": "6e17dfa9-6ad9-47eb-fd0f-d700d78f291a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "648032caa94f4d5bb2593c892bd54754"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/286 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3e0d7d1e7624c8cb6c6daa14138c43c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets & Dataloaders\n"
      ],
      "metadata": {
        "id": "CgQdrl2pQkno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64"
      ],
      "metadata": {
        "id": "KU-tl0cIRFY-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collator(batch, tokenizer, eval=False, doc_position=False):\n",
        "    # standard bert collator that applies the right label for the training task\n",
        "    sents = []\n",
        "    labels = []\n",
        "    doc_poses = []\n",
        "    doc_ids = []\n",
        "    sent_ixs = []\n",
        "    for sent, label, doc_id, sent_ix in batch:\n",
        "        sents.append(' '.join(sent))\n",
        "        label = multilabel_labeler(label)\n",
        "        labels.append(label)\n",
        "        doc_ids.append(doc_id)\n",
        "        sent_ixs.append(sent_ix)\n",
        "    tokd = tokenizer(sents, padding=True, max_length = 512, truncation=True)\n",
        "    input_ids, token_type_ids, attention_mask = tokd['input_ids'], tokd['token_type_ids'], tokd['attention_mask']\n",
        "    toks = torch.LongTensor(input_ids)\n",
        "    mask = torch.LongTensor(attention_mask)\n",
        "    labels = torch.Tensor(labels)\n",
        "    # for whatever reason, pytorch's cross entropy requires long labels but BCE doesn't\n",
        "    if not eval:\n",
        "        return {'input_ids': toks, 'attention_mask': mask, 'labels': labels, 'doc_ids': doc_ids, 'sent_ixs': sent_ixs}\n",
        "    else:\n",
        "        return {'input_ids': toks, 'attention_mask': mask, 'labels': labels, 'sentences': sents, 'doc_ids': doc_ids, 'sent_ixs': sent_ixs}\n",
        "\n",
        "data_collator = lambda x: collator(x, tokenizer = tokenizer1)\n",
        "eval_collate_fn = lambda x: collator(x, tokenizer =tokenizer1, eval=True)"
      ],
      "metadata": {
        "id": "Tcr3dZOuwJCG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import tqdm\n",
        "\n",
        "# class BertDataset(torch.utils.data.Dataset):\n",
        "#   def __init__(self, txt_list, tokenizer, max_length=512):\n",
        "#     self.tokenizer = tokenizer\n",
        "#     self.sents = []\n",
        "#     self.labels = []\n",
        "#     self.doc_poses = []\n",
        "#     self.doc_ids = []\n",
        "#     self.sent_ixs = []\n",
        "#     self.input_ids = []\n",
        "#     self.token_type_ids = []\n",
        "#     self.attention_maks = []\n",
        "\n",
        "\n",
        "#     for doc_id, sent_ix, sent, label in tqdm.tqdm(txt_list.values, desc=\"Tokenizing data\"):\n",
        "#         #sents.append(' '.join(sent))\n",
        "#         tokd = tokenizer(sent, padding=True, max_length = 512)\n",
        "#         self.input_ids.append(torch.Tensor(tokd['input_ids']))\n",
        "#         self.token_type_ids.append(tokd['token_type_ids'])\n",
        "#         self.attention_maks.append(torch.Tensor(tokd['attention_mask']))\n",
        "#         label = multilabel_labeler(label)\n",
        "#         self.labels.append(label)\n",
        "#         self.doc_ids.append(doc_id)\n",
        "#         self.sent_ixs.append(sent_ix)\n",
        "#     self.labels = torch.Tensor(self.labels)\n",
        "    \n",
        "#   def __len__(self):\n",
        "#     return len(self.sent_ixs)\n",
        "\n",
        "#   def __getitem__(self, idx):\n",
        "#     return {'input_ids': self.input_ids[idx], 'attention_mask': self.attention_maks[idx], 'labels': self.labels[idx], 'doc_ids': self.doc_ids[idx], 'sent_ixs': self.sent_ixs[idx]} "
      ],
      "metadata": {
        "id": "PF8HQ_vbSHm5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL_TYPES = ['I-Imaging-related followup',\n",
        " 'I-Appointment-related followup',\n",
        " 'I-Medication-related followups',\n",
        " 'I-Procedure-related followup',\n",
        " 'I-Lab-related followup',\n",
        " 'I-Case-specific instructions for patient',\n",
        " 'I-Other helpful contextual information',\n",
        " ]\n",
        "\n",
        "label2abbrev = {'I-Imaging-related followup': 'Imaging',\n",
        "        'I-Appointment-related followup': 'Appointment',\n",
        "        'I-Medication-related followups': 'Medication',\n",
        "        'I-Procedure-related followup': 'Procedure',\n",
        "        'I-Lab-related followup': 'Lab',\n",
        "        'I-Case-specific instructions for patient': 'Patient instructions',\n",
        "        'I-Other helpful contextual information': 'Other',\n",
        " }\n",
        "\n",
        "def multilabel_labeler(label):\n",
        "    # convert list of label names to a multi-hot array\n",
        "    if isinstance(label, str):\n",
        "        label = eval(label)\n",
        "    label_ixs = [LABEL_TYPES.index(l) for l in label]\n",
        "    label = np.zeros(len(LABEL_TYPES))\n",
        "    label[label_ixs] = 1\n",
        "    return label"
      ],
      "metadata": {
        "id": "lKjcgW8Ryz8N"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# train_dataset = BertDataset(train_dataset, tokenizer, max_length=512, collate_fn=data_collator)\n",
        "# eval_dataset = BertDataset(eval_dataset, tokenizer, max_length=512, collate_fn=eval_collate_fn)\n",
        "\n",
        "# print()\n",
        "# print('{:>5,} training samples'.format(len(train_dataset)))\n",
        "# print('{:>5,} validation samples'.format(len(eval_dataset)))"
      ],
      "metadata": {
        "id": "f21UWyng9-Al"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "  train_dataset,\n",
        "  batch_size=batch_size,\n",
        "  collate_fn=data_collator\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "  eval_dataset,\n",
        "  shuffle=False,\n",
        "  batch_size=1,\n",
        "  collate_fn=eval_collate_fn\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=1, shuffle=False, collate_fn=eval_collate_fn\n",
        "    )"
      ],
      "metadata": {
        "id": "A4BrtN3cXp_y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune BERT Model"
      ],
      "metadata": {
        "id": "h2OVhp8rFGA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertPreTrainedModel, BertModel, BertForMaskedLM\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss, KLDivLoss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MiniBertForSequenceMultilabelClassification(BertPreTrainedModel):\n",
        "    \"\"\"\n",
        "        simple mod of MiniBERT to accept multilabel or binary task\n",
        "        there may or may not be a class in huggingface that does this but this is here for historical reasons\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.minibert = BertModel.from_pretrained('prajjwal1/bert-mini')\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def add_doc_position_feature(self, config):\n",
        "        self.classifier = nn.Linear(config.hidden_size+1, config.num_labels)\n",
        "\n",
        "    def set_task(self, task):\n",
        "        self.task = task\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        doc_positions=None,\n",
        "    ):\n",
        "        outputs = self.minibert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        if doc_positions is not None and len(doc_positions) > 0:\n",
        "            pooled_output = torch.cat((pooled_output, doc_positions.unsqueeze(1)), dim=1)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = F.binary_cross_entropy_with_logits(outputs[0], labels)\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lv38x2zLTmJo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_set = LABEL_TYPES\n",
        "num_labels = len(label_set)\n",
        "\n",
        "label2id = {label:ix for ix,label in enumerate(label_set)}\n",
        "id2label = {ix:label for label,ix in label2id.items()}\n",
        "config = BertConfig.from_pretrained(\n",
        "        'prajjwal1/bert-mini',\n",
        "        num_labels=num_labels,\n",
        "        finetuning_task=\"text_classification\",\n",
        "        label2id=label2id,\n",
        "        id2label=id2label,\n",
        "    )"
      ],
      "metadata": {
        "id": "bs7AN7UKHy5A"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "model = MiniBertForSequenceMultilabelClassification.from_pretrained('prajjwal1/bert-mini', config=config)\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "16bd5ed102ab4a199fdf62957fe562db",
            "5d6d95e6d24d480e9bbf09a96e70c1fc",
            "d79a10e6eebc48b99d2748913a6d9b6b",
            "49474748743047248350fa27eb145f91",
            "01a5fff3fea74c448f0ee838e7d1a652",
            "fe10fa7cbb604a4384ee9e16ade348f8",
            "7234bcd5375245e08b68d8d42ea86f1f",
            "a968a8e6e97140f4836d8699cf848535",
            "2c3c8bf6d0774882bddd7773fad63be2",
            "1b0fd797bbf64067b32dfccac2e60f76",
            "d4b23997ee294341a2e241654614d4a7"
          ]
        },
        "id": "2B-HztvrJs7Q",
        "outputId": "2faa545c-304a-4a1a-f214-e69c0adc6ee3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/45.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16bd5ed102ab4a199fdf62957fe562db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing MiniBertForSequenceMultilabelClassification: ['bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'cls.predictions.transform.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'cls.predictions.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.position_ids', 'bert.pooler.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.embeddings.LayerNorm.bias', 'cls.predictions.decoder.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'cls.seq_relationship.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.key.bias']\n",
            "- This IS expected if you are initializing MiniBertForSequenceMultilabelClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing MiniBertForSequenceMultilabelClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of MiniBertForSequenceMultilabelClassification were not initialized from the model checkpoint at prajjwal1/bert-mini and are newly initialized: ['bert.minibert.embeddings.token_type_embeddings.weight', 'bert.minibert.encoder.layer.1.attention.self.value.bias', 'bert.minibert.encoder.layer.1.attention.self.key.weight', 'bert.minibert.encoder.layer.1.output.LayerNorm.weight', 'bert.minibert.encoder.layer.2.attention.self.query.bias', 'bert.minibert.encoder.layer.1.attention.output.dense.weight', 'bert.minibert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.minibert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.minibert.encoder.layer.1.attention.self.query.weight', 'bert.minibert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.minibert.encoder.layer.1.intermediate.dense.weight', 'bert.minibert.pooler.dense.bias', 'bert.minibert.encoder.layer.1.attention.self.query.bias', 'bert.minibert.encoder.layer.3.output.LayerNorm.bias', 'bert.minibert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.classifier.bias', 'bert.minibert.encoder.layer.2.output.dense.weight', 'bert.minibert.encoder.layer.3.attention.self.key.bias', 'bert.minibert.encoder.layer.0.attention.self.query.weight', 'bert.minibert.encoder.layer.2.intermediate.dense.bias', 'bert.minibert.encoder.layer.1.intermediate.dense.bias', 'bert.minibert.encoder.layer.3.attention.output.dense.weight', 'bert.minibert.encoder.layer.2.output.dense.bias', 'bert.minibert.encoder.layer.3.intermediate.dense.weight', 'bert.minibert.encoder.layer.3.attention.self.key.weight', 'bert.minibert.pooler.dense.weight', 'bert.minibert.encoder.layer.0.attention.self.key.bias', 'bert.minibert.encoder.layer.0.output.LayerNorm.bias', 'bert.minibert.encoder.layer.2.output.LayerNorm.bias', 'bert.minibert.encoder.layer.3.output.dense.bias', 'bert.minibert.embeddings.word_embeddings.weight', 'bert.minibert.embeddings.LayerNorm.bias', 'bert.minibert.embeddings.position_embeddings.weight', 'bert.minibert.encoder.layer.0.attention.self.value.weight', 'bert.minibert.encoder.layer.3.attention.self.query.weight', 'bert.minibert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.minibert.encoder.layer.3.attention.self.query.bias', 'bert.minibert.encoder.layer.1.attention.output.dense.bias', 'bert.minibert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.minibert.encoder.layer.2.attention.self.key.weight', 'bert.minibert.encoder.layer.2.attention.self.value.bias', 'bert.minibert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.minibert.encoder.layer.2.attention.self.query.weight', 'bert.minibert.encoder.layer.1.output.dense.weight', 'bert.minibert.encoder.layer.2.output.LayerNorm.weight', 'bert.minibert.encoder.layer.0.output.LayerNorm.weight', 'bert.minibert.encoder.layer.2.attention.self.key.bias', 'bert.minibert.encoder.layer.3.output.dense.weight', 'bert.minibert.encoder.layer.3.attention.self.value.weight', 'bert.minibert.encoder.layer.3.attention.output.dense.bias', 'bert.minibert.encoder.layer.0.attention.self.query.bias', 'bert.minibert.encoder.layer.0.attention.self.key.weight', 'bert.minibert.encoder.layer.0.attention.output.dense.bias', 'bert.minibert.encoder.layer.1.attention.self.value.weight', 'bert.minibert.encoder.layer.3.output.LayerNorm.weight', 'bert.minibert.encoder.layer.2.attention.output.dense.weight', 'bert.minibert.encoder.layer.2.attention.output.dense.bias', 'bert.minibert.encoder.layer.3.intermediate.dense.bias', 'bert.minibert.encoder.layer.0.intermediate.dense.bias', 'bert.minibert.encoder.layer.0.attention.output.dense.weight', 'bert.minibert.encoder.layer.1.output.dense.bias', 'bert.minibert.encoder.layer.2.intermediate.dense.weight', 'bert.minibert.encoder.layer.2.attention.self.value.weight', 'bert.classifier.weight', 'bert.minibert.encoder.layer.0.output.dense.weight', 'bert.minibert.encoder.layer.1.attention.self.key.bias', 'bert.minibert.encoder.layer.0.attention.self.value.bias', 'bert.minibert.encoder.layer.0.intermediate.dense.weight', 'bert.minibert.encoder.layer.0.output.dense.bias', 'bert.minibert.encoder.layer.3.attention.self.value.bias', 'bert.minibert.embeddings.LayerNorm.weight', 'bert.minibert.encoder.layer.1.output.LayerNorm.bias', 'bert.minibert.encoder.layer.3.attention.output.LayerNorm.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "freeze weights if needed"
      ],
      "metadata": {
        "id": "lfF5PFP4XkPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for name, params in model.named_parameters():\n",
        "#   if name.startswith('bert.embeddings.'):\n",
        "#     params.requires_grad = False\n",
        "#   if name.startswith('bert.encoder.layer.0'):\n",
        "#     params.requires_grad = False\n",
        "#   if name.startswith('bert.encoder.layer.1'):\n",
        "#     params.requires_grad = False\n",
        "#   if name.startswith('bert.encoder.layer.2'):\n",
        "#     params.requires_grad = False\n",
        "#   if name.startswith('bert.encoder.layer.3'):\n",
        "#     params.requires_grad = False\n",
        "#   if name.startswith('bert.encoder.layer.4'):\n",
        "#     params.requires_grad = False\n",
        "#   if name.startswith('bert.encoder.layer.5'):\n",
        "#     params.requires_grad = False\n",
        "#   if name.startswith('bert.encoder.layer.6'):\n",
        "#     params.requires_grad = False\n",
        "#   if name.startswith('bert.encoder.layer.7'):\n",
        "#     params.requires_grad = False\n",
        "#   if name.startswith('bert.encoder.layer.8'):\n",
        "#     params.requires_grad = False\n",
        "#   if name.startswith('bert.encoder.layer.9'):\n",
        "#     params.requires_grad = False\n",
        "#   if name.startswith('bert.encoder.layer.10'):\n",
        "#     params.requires_grad = False\n",
        "#   if name.startswith('bert.encoder.layer.11'):\n",
        "#     params.requires_grad = False"
      ],
      "metadata": {
        "id": "epNJ1zr8_-EF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "timestamp = time.strftime('%b_%d_%H:%M:%S', time.localtime())\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YZDa2pVUipQ",
        "outputId": "1ba213a2-d8e1-47da-89d8-5b249dcd2f92"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Predictions"
      ],
      "metadata": {
        "id": "qleopQt9fc4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gather_predictions(model, loader, device, doc_position=False, save_preds=False, max_preds=1e9):\n",
        "    # run through data loader and get model predictions\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        yhat_raw = np.zeros((len(loader), 7))\n",
        "        yhat = np.zeros((len(loader), 7))\n",
        "        y = np.zeros((len(loader), 7))\n",
        "        sentences = []\n",
        "        doc_ids = []\n",
        "        sent_ixs = []\n",
        "        for ix, x in enumerate(loader):\n",
        "            if ix >= max_preds:\n",
        "                break\n",
        "            for k, v in x.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    x[k] = v.to(device)\n",
        "            sentences.extend(x['sentences'])\n",
        "            doc_ids.extend(x['doc_ids'])\n",
        "            sent_ixs.extend(x['sent_ixs'])\n",
        "            inputs = {'input_ids': x['input_ids'], 'attention_mask': x['attention_mask'], 'labels': x['labels']}\n",
        "            if 'token_type_ids' in x:\n",
        "              inputs['token_type_ids'] = x['token_type_ids']\n",
        "            loss, pred = model(**inputs)\n",
        "            pred = torch.sigmoid(pred)\n",
        "            pred = pred.cpu().numpy()\n",
        "            yhat_raw[ix] = pred\n",
        "            yhat[ix] = np.round(pred)\n",
        "            y[ix] = x['labels'].cpu().numpy()[0]\n",
        "    return yhat_raw, yhat, y, sentences, doc_ids, sent_ixs"
      ],
      "metadata": {
        "id": "4Sg_mFp-VIXq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrics"
      ],
      "metadata": {
        "id": "m5vysVhefOOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import time\n",
        "\n",
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ],
      "metadata": {
        "id": "uuZdTrAJlg5p"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import csv\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def auc_metrics(yhat_raw, y, ymic):\n",
        "    if yhat_raw.shape[0] <= 1:\n",
        "        return\n",
        "    fpr = {}\n",
        "    tpr = {}\n",
        "    roc_auc = {}\n",
        "    #get AUC for each label individually\n",
        "    relevant_labels = []\n",
        "    auc_labels = {}\n",
        "    for i in range(y.shape[1]):\n",
        "        #only if there are true positives for this label\n",
        "        if y[:,i].sum() > 0:\n",
        "            fpr[i], tpr[i], _ = roc_curve(y[:,i], yhat_raw[:,i])\n",
        "            if len(fpr[i]) > 1 and len(tpr[i]) > 1:\n",
        "                auc_score = auc(fpr[i], tpr[i])\n",
        "                if not np.isnan(auc_score): \n",
        "                    auc_labels[\"auc_%d\" % i] = auc_score\n",
        "                    relevant_labels.append(i)\n",
        "\n",
        "    #macro-AUC: just average the auc scores\n",
        "    aucs = []\n",
        "    for i in relevant_labels:\n",
        "        aucs.append(auc_labels['auc_%d' % i])\n",
        "    roc_auc['auc_macro'] = np.mean(aucs)\n",
        "\n",
        "    #micro-AUC: just look at each individual prediction\n",
        "    yhatmic = yhat_raw.ravel()\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ymic, yhatmic) \n",
        "    roc_auc[\"auc_micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "    return roc_auc\n",
        "\n",
        "def union_size(yhat, y, axis):\n",
        "    #axis=0 for label-level union (macro). axis=1 for instance-level\n",
        "    return np.logical_or(yhat, y).sum(axis=axis).astype(float)\n",
        "\n",
        "def intersect_size(yhat, y, axis):\n",
        "    #axis=0 for label-level union (macro). axis=1 for instance-level\n",
        "    return np.logical_and(yhat, y).sum(axis=axis).astype(float)\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "#MACRO METRICS: calculate metric for each label and average across labels\n",
        "#########################################################################\n",
        "\n",
        "def macro_accuracy(yhat, y):\n",
        "    num = intersect_size(yhat, y, 0) / (union_size(yhat, y, 0) + 1e-10)\n",
        "    return np.mean(num)\n",
        "\n",
        "def macro_precision(yhat, y):\n",
        "    num = intersect_size(yhat, y, 0) / (yhat.sum(axis=0) + 1e-10)\n",
        "    return np.mean(num)\n",
        "\n",
        "def macro_recall(yhat, y):\n",
        "    num = intersect_size(yhat, y, 0) / (y.sum(axis=0) + 1e-10)\n",
        "    return np.mean(num)\n",
        "\n",
        "def macro_f1(yhat, y):\n",
        "    prec = macro_precision(yhat, y)\n",
        "    rec = macro_recall(yhat, y)\n",
        "    if prec + rec == 0:\n",
        "        f1 = 0.\n",
        "    else:\n",
        "        f1 = 2*(prec*rec)/(prec+rec)\n",
        "    return f1\n",
        "\n",
        "##########################################################################\n",
        "#MICRO METRICS: treat every prediction as an individual binary prediction\n",
        "##########################################################################\n",
        "\n",
        "def micro_accuracy(yhatmic, ymic):\n",
        "    return intersect_size(yhatmic, ymic, 0) / union_size(yhatmic, ymic, 0)\n",
        "\n",
        "def micro_precision(yhatmic, ymic):\n",
        "    return intersect_size(yhatmic, ymic, 0) / yhatmic.sum(axis=0)\n",
        "\n",
        "def micro_recall(yhatmic, ymic):\n",
        "    return intersect_size(yhatmic, ymic, 0) / ymic.sum(axis=0)\n",
        "\n",
        "def micro_f1(yhatmic, ymic):\n",
        "    prec = micro_precision(yhatmic, ymic)\n",
        "    rec = micro_recall(yhatmic, ymic)\n",
        "    if prec + rec == 0:\n",
        "        f1 = 0.\n",
        "    else:\n",
        "        f1 = 2*(prec*rec)/(prec+rec)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def all_macro(yhat, y):\n",
        "    return macro_accuracy(yhat, y), macro_precision(yhat, y), macro_recall(yhat, y), macro_f1(yhat, y)\n",
        "\n",
        "def all_micro(yhatmic, ymic):\n",
        "    return micro_accuracy(yhatmic, ymic), micro_precision(yhatmic, ymic), micro_recall(yhatmic, ymic), micro_f1(yhatmic, ymic)\n",
        "\n",
        "\n",
        "def all_metrics(yhat, y, yhat_raw=None, calc_auc=True, label_order=[]):\n",
        "    \"\"\"\n",
        "        Inputs:\n",
        "            yhat: binary predictions matrix \n",
        "            y: binary ground truth matrix\n",
        "            yhat_raw: prediction scores matrix (floats)\n",
        "        Outputs:\n",
        "            dict holding relevant metrics\n",
        "    \"\"\"\n",
        "    names = [\"acc\", \"prec\", \"rec\", \"f1\"]\n",
        "\n",
        "    metrics = {}\n",
        "    for ix,label in enumerate(label_order):\n",
        "        metrics[f\"{label2abbrev[label]}-f1\"] = f1_score(y[:,ix], yhat[:,ix])\n",
        "\n",
        "    #macro\n",
        "    #print(\"GETTING ALL MACRO\")\n",
        "    macro = all_macro(yhat, y)\n",
        "\n",
        "    #micro\n",
        "    #print(\"GETTING ALL MICRO\")\n",
        "    ymic = y.ravel()\n",
        "    yhatmic = yhat.ravel()\n",
        "    micro = all_micro(yhatmic, ymic)\n",
        "\n",
        "    metrics.update({names[i] + \"_macro\": macro[i] for i in range(len(macro))})\n",
        "    metrics.update({names[i] + \"_micro\": micro[i] for i in range(len(micro))})\n",
        "\n",
        "    #AUC\n",
        "    #print(\"AUC\")\n",
        "    if yhat_raw is not None and calc_auc:\n",
        "        roc_auc = auc_metrics(yhat_raw, y, ymic)\n",
        "        metrics.update(roc_auc)\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "JfZ6qMlfd48u"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dv_loader, device, tokenizer, return_thresholds=False, doc_position=False):\n",
        "    # apply model to validation set and compute metrics, including identifying best thresholds\n",
        "    yhat_raw, yhat, y, sentences, doc_ids, sent_ixs = gather_predictions(model, dv_loader, device)\n",
        "\n",
        "\n",
        "    # unbalanced metrics\n",
        "    metrics = all_metrics(yhat, y, yhat_raw=yhat_raw, calc_auc=True, label_order=LABEL_TYPES)\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "mknFQXXeY0ks"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "\n",
        "max_epochs=5\n",
        "learning_rate=5e-5\n",
        "max_steps=-1\n",
        "gradient_accumulation_steps=4\n",
        "eval_steps = 100\n"
      ],
      "metadata": {
        "id": "kerxyrgGGTuY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "total_t0 = time.time()\n",
        "tr_loss = 0.0\n",
        "model.zero_grad()\n",
        "model.train()\n",
        "metrics_hist = defaultdict(list)\n",
        "best_epoch = 0\n",
        "best_step = 0\n",
        "step = 0\n",
        "losses = []\n",
        "for epoch in range(max_epochs):\n",
        "        step = 0\n",
        "        t0 = time.time()\n",
        "        for x in tqdm(train_dataloader):\n",
        "            if max_steps > -1 and step > max_steps:\n",
        "                break\n",
        "            # transfer to gpu\n",
        "            for k, v in x.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    x[k] = v.to(device)\n",
        "            inputs = {'input_ids': x['input_ids'], 'attention_mask': x['attention_mask'], 'labels': x['labels']}\n",
        "            if 'token_type_ids' in x:\n",
        "              inputs['token_type_ids'] = x['token_type_ids']\n",
        "            #outputs = model(**inputs)\n",
        "            loss, pred = model(**inputs)\n",
        "\n",
        "            # update parameters\n",
        "            if gradient_accumulation_steps > 1:\n",
        "                loss = loss / gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % gradient_accumulation_steps == 0 or (\n",
        "                # last step in epoch but step is always smaller than gradient_accumulation_steps\n",
        "                len(train_dataloader) <= gradient_accumulation_steps\n",
        "                and (step + 1) == len(train_dataloader)\n",
        "            ):\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                model.zero_grad()\n",
        "                losses.append(tr_loss)\n",
        "                tr_loss = 0.0\n",
        "\n",
        "            # Validation\n",
        "            if (step + 1) == len(train_dataloader):\n",
        "                metrics = evaluate(model, validation_dataloader, device, tokenizer=tokenizer1)\n",
        "                for name, metric in metrics.items():\n",
        "                    metrics_hist[name].append(metric)\n",
        "\n",
        "            # tensor output\n",
        "            if (step + 1) % eval_steps == 0:\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), np.mean(losses[-10:]), elapsed))\n",
        "\n",
        "            step += 1\n",
        "\n",
        "#evaluation metrics\n",
        "metrics = evaluate(model, validation_dataloader, device, tokenizer=tokenizer1)\n",
        "for name, metric in metrics.items():\n",
        "  metrics_hist[name].append(metric)\n",
        "\n",
        "#test metrics\n",
        "metrics_hist_test = defaultdict(list)\n",
        "metrics_test = evaluate(model, test_dataloader, device, tokenizer=tokenizer1)\n",
        "for name, metric in metrics_test.items():\n",
        "  metrics_hist_test[name].append(metric)\n",
        "\n",
        "#save model\n",
        "out_dir = \"/content/drive/My Drive/Deep Learning Project\"\n",
        "sd = model.state_dict()\n",
        "torch.save(sd, out_dir + \"/model_final.pth\")\n",
        "\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGn65XaNTPNt",
        "outputId": "e858dfb3-7dda-4e1b-cd47-470dbb4fed83"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1198 [00:00<?, ?it/s]<ipython-input-10-ec9cf2df673a>:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  labels = torch.Tensor(labels)\n",
            "  9%|▊         | 102/1198 [00:10<01:21, 13.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    99  of  1,198. Loss: 0.46003335192799566.   Elapsed: 0:00:10.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 201/1198 [00:16<00:58, 16.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   199  of  1,198. Loss: 0.32514231353998185.   Elapsed: 0:00:16.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 302/1198 [00:22<00:50, 17.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   299  of  1,198. Loss: 0.23087943755090237.   Elapsed: 0:00:22.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 404/1198 [00:28<00:42, 18.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   399  of  1,198. Loss: 0.17209042347967624.   Elapsed: 0:00:28.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 501/1198 [00:34<00:38, 17.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   499  of  1,198. Loss: 0.14792772121727465.   Elapsed: 0:00:34.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 601/1198 [00:40<00:44, 13.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   599  of  1,198. Loss: 0.12727501448243855.   Elapsed: 0:00:41.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▉    | 704/1198 [00:47<00:25, 19.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   699  of  1,198. Loss: 0.11654797904193401.   Elapsed: 0:00:47.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 804/1198 [00:53<00:22, 17.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   799  of  1,198. Loss: 0.1110987632535398.   Elapsed: 0:00:53.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 903/1198 [00:59<00:18, 15.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   899  of  1,198. Loss: 0.10441991426050663.   Elapsed: 0:01:00.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 1003/1198 [01:05<00:11, 17.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   999  of  1,198. Loss: 0.10269517907872797.   Elapsed: 0:01:05.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 1102/1198 [01:11<00:05, 17.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 1,099  of  1,198. Loss: 0.09712152490392327.   Elapsed: 0:01:11.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 1196/1198 [01:30<00:00, 15.31it/s]<ipython-input-22-36a466424a07>:86: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return intersect_size(yhatmic, ymic, 0) / yhatmic.sum(axis=0)\n",
            "100%|██████████| 1198/1198 [02:39<00:00,  7.52it/s]\n",
            "  8%|▊         | 101/1198 [00:06<01:21, 13.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    99  of  1,198. Loss: 0.09043871210888028.   Elapsed: 0:00:06.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 201/1198 [00:12<00:57, 17.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   199  of  1,198. Loss: 0.0775149367749691.   Elapsed: 0:00:12.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 302/1198 [00:18<00:50, 17.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   299  of  1,198. Loss: 0.06986751011572778.   Elapsed: 0:00:19.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 403/1198 [00:24<00:40, 19.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   399  of  1,198. Loss: 0.06225367519073188.   Elapsed: 0:00:25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 501/1198 [00:30<00:38, 18.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   499  of  1,198. Loss: 0.0651960332877934.   Elapsed: 0:00:31.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 601/1198 [00:37<00:42, 14.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   599  of  1,198. Loss: 0.06143377721309662.   Elapsed: 0:00:37.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▉    | 704/1198 [00:43<00:25, 19.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   699  of  1,198. Loss: 0.05773179749958217.   Elapsed: 0:00:43.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 804/1198 [00:49<00:21, 18.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   799  of  1,198. Loss: 0.05993672697804868.   Elapsed: 0:00:49.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 903/1198 [00:55<00:18, 15.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   899  of  1,198. Loss: 0.054523811349645256.   Elapsed: 0:00:55.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 1003/1198 [01:01<00:10, 17.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   999  of  1,198. Loss: 0.059010479226708414.   Elapsed: 0:01:01.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 1102/1198 [01:07<00:05, 17.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 1,099  of  1,198. Loss: 0.05450966497883201.   Elapsed: 0:01:07.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1198/1198 [02:35<00:00,  7.68it/s]\n",
            "  9%|▊         | 102/1198 [00:06<01:19, 13.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    99  of  1,198. Loss: 0.060077344975434245.   Elapsed: 0:00:06.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 201/1198 [00:12<00:58, 17.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   199  of  1,198. Loss: 0.04935198244638741.   Elapsed: 0:00:13.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 302/1198 [00:18<00:50, 17.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   299  of  1,198. Loss: 0.046334127662703395.   Elapsed: 0:00:19.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 404/1198 [00:24<00:41, 18.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   399  of  1,198. Loss: 0.04110756819136441.   Elapsed: 0:00:25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 501/1198 [00:30<00:38, 17.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   499  of  1,198. Loss: 0.04618859279435128.   Elapsed: 0:00:31.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 601/1198 [00:37<00:42, 14.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   599  of  1,198. Loss: 0.04431038638576865.   Elapsed: 0:00:37.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▊    | 703/1198 [00:43<00:26, 18.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   699  of  1,198. Loss: 0.03930155953858048.   Elapsed: 0:00:43.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 804/1198 [00:49<00:21, 17.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   799  of  1,198. Loss: 0.04316351932939142.   Elapsed: 0:00:49.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 904/1198 [00:55<00:17, 16.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   899  of  1,198. Loss: 0.04002765074837953.   Elapsed: 0:00:55.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 1003/1198 [01:01<00:11, 16.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   999  of  1,198. Loss: 0.04487797438632697.   Elapsed: 0:01:01.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 1102/1198 [01:07<00:05, 17.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 1,099  of  1,198. Loss: 0.042673731897957624.   Elapsed: 0:01:07.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1198/1198 [02:35<00:00,  7.71it/s]\n",
            "  8%|▊         | 101/1198 [00:06<01:20, 13.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    99  of  1,198. Loss: 0.04735151205677539.   Elapsed: 0:00:06.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 201/1198 [00:12<00:59, 16.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   199  of  1,198. Loss: 0.03821628113510087.   Elapsed: 0:00:13.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 302/1198 [00:18<00:50, 17.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   299  of  1,198. Loss: 0.035887209267821164.   Elapsed: 0:00:19.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 402/1198 [00:24<00:40, 19.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   399  of  1,198. Loss: 0.03344026461709291.   Elapsed: 0:00:25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 501/1198 [00:30<00:38, 18.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   499  of  1,198. Loss: 0.03788269255310297.   Elapsed: 0:00:31.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 601/1198 [00:37<00:43, 13.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   599  of  1,198. Loss: 0.037077158188913016.   Elapsed: 0:00:37.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▉    | 704/1198 [00:43<00:25, 19.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   699  of  1,198. Loss: 0.030459912167862056.   Elapsed: 0:00:43.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 804/1198 [00:49<00:21, 18.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   799  of  1,198. Loss: 0.03533502211794257.   Elapsed: 0:00:49.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 903/1198 [00:55<00:18, 15.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   899  of  1,198. Loss: 0.03329079230315983.   Elapsed: 0:00:56.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 1003/1198 [01:01<00:10, 17.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   999  of  1,198. Loss: 0.037722500471863894.   Elapsed: 0:01:01.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 1102/1198 [01:07<00:05, 17.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 1,099  of  1,198. Loss: 0.036434451804962006.   Elapsed: 0:01:07.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1198/1198 [02:35<00:00,  7.70it/s]\n",
            "  8%|▊         | 101/1198 [00:06<01:22, 13.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    99  of  1,198. Loss: 0.03858049339614809.   Elapsed: 0:00:06.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 201/1198 [00:12<00:58, 17.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   199  of  1,198. Loss: 0.03265303391963244.   Elapsed: 0:00:12.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 301/1198 [00:18<00:55, 16.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   299  of  1,198. Loss: 0.029785696184262633.   Elapsed: 0:00:19.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 404/1198 [00:25<00:42, 18.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   399  of  1,198. Loss: 0.028280473593622447.   Elapsed: 0:00:25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 501/1198 [00:30<00:39, 17.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   499  of  1,198. Loss: 0.03201115753035992.   Elapsed: 0:00:31.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 601/1198 [00:37<00:42, 14.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   599  of  1,198. Loss: 0.03135218098759651.   Elapsed: 0:00:37.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▉    | 704/1198 [00:43<00:25, 19.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   699  of  1,198. Loss: 0.024839116737712174.   Elapsed: 0:00:43.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 804/1198 [00:49<00:21, 18.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   799  of  1,198. Loss: 0.03073982266942039.   Elapsed: 0:00:49.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 903/1198 [00:55<00:19, 15.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   899  of  1,198. Loss: 0.028488232346717268.   Elapsed: 0:00:56.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 1003/1198 [01:01<00:11, 17.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   999  of  1,198. Loss: 0.03144577117054723.   Elapsed: 0:01:02.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 1102/1198 [01:07<00:05, 17.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 1,099  of  1,198. Loss: 0.032513184973504396.   Elapsed: 0:01:07.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1198/1198 [02:35<00:00,  7.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics)\n",
        "print(metrics_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tViVfouZhryo",
        "outputId": "a2e86503-1e37-476f-f796-3f0d9c8ef7fd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Imaging-f1': 0.0, 'Appointment-f1': 0.829736211031175, 'Medication-f1': 0.4792079207920792, 'Procedure-f1': 0.0, 'Lab-f1': 0.0847457627118644, 'Patient instructions-f1': 0.8197981739548295, 'Other-f1': 0.0, 'acc_macro': 0.25185625069796547, 'prec_macro': 0.47723877511785506, 'rec_macro': 0.29646726552989827, 'f1_macro': 0.36573496193871124, 'acc_micro': 0.5752688172043011, 'prec_micro': 0.8185792349726776, 'rec_micro': 0.659330985915493, 'f1_micro': 0.7303754266211604, 'auc_macro': 0.9343423172534683, 'auc_micro': 0.9664496720465994}\n",
            "{'Imaging-f1': 0.0, 'Appointment-f1': 0.8344370860927153, 'Medication-f1': 0.5092838196286472, 'Procedure-f1': 0.0, 'Lab-f1': 0.08695652173913045, 'Patient instructions-f1': 0.7624576681180454, 'Other-f1': 0.0, 'acc_macro': 0.2455867114446016, 'prec_macro': 0.48127934175860215, 'rec_macro': 0.2842362304799192, 'f1_macro': 0.35739841453335774, 'acc_micro': 0.5529411764705883, 'prec_micro': 0.8292544109277177, 'rec_micro': 0.623982869379015, 'f1_micro': 0.7121212121212123, 'auc_macro': 0.9303305074547497, 'auc_micro': 0.9621416736776457}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Bert Model"
      ],
      "metadata": {
        "id": "RpXCjgvce4zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForSequenceMultilabelClassification(BertPreTrainedModel):\n",
        "    \"\"\"\n",
        "        simple mod of MiniBERT to accept multilabel or binary task\n",
        "        there may or may not be a class in huggingface that does this but this is here for historical reasons\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def add_doc_position_feature(self, config):\n",
        "        self.classifier = nn.Linear(config.hidden_size+1, config.num_labels)\n",
        "\n",
        "    def set_task(self, task):\n",
        "        self.task = task\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        doc_positions=None,\n",
        "    ):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        if doc_positions is not None and len(doc_positions) > 0:\n",
        "            pooled_output = torch.cat((pooled_output, doc_positions.unsqueeze(1)), dim=1)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = F.binary_cross_entropy_with_logits(outputs[0], labels)\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "KIOlpAIae3-L"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1 = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "data_collator = lambda x: collator(x, tokenizer = tokenizer1)\n",
        "eval_collate_fn = lambda x: collator(x, tokenizer =tokenizer1, eval=True)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "  train_dataset,\n",
        "  batch_size=batch_size,\n",
        "  collate_fn=data_collator\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "  eval_dataset,\n",
        "  shuffle=False,\n",
        "  batch_size=1,\n",
        "  collate_fn=eval_collate_fn\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=1, shuffle=False, collate_fn=eval_collate_fn\n",
        "    )"
      ],
      "metadata": {
        "id": "j3oLEFnbfRDx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = BertConfig.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels=num_labels,\n",
        "        finetuning_task=\"text_classification\",\n",
        "        label2id=label2id,\n",
        "        id2label=id2label,\n",
        "    )\n",
        "\n",
        "\n",
        "model = BertForSequenceMultilabelClassification.from_pretrained('bert-base-uncased', config=config)\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHRwrzoCgCza",
        "outputId": "0cae339a-d344-4f9b-e245-bc3e3a564318"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceMultilabelClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceMultilabelClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceMultilabelClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceMultilabelClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "Uted9SXgga2d"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "total_t0 = time.time()\n",
        "tr_loss = 0.0\n",
        "model.zero_grad()\n",
        "model.train()\n",
        "metrics_hist_fullBert = defaultdict(list)\n",
        "best_epoch = 0\n",
        "best_step = 0\n",
        "step = 0\n",
        "losses = []\n",
        "for epoch in range(max_epochs):\n",
        "        step = 0\n",
        "        t0 = time.time()\n",
        "        for x in tqdm(train_dataloader):\n",
        "            if max_steps > -1 and step > max_steps:\n",
        "                break\n",
        "            # transfer to gpu\n",
        "            for k, v in x.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    x[k] = v.to(device)\n",
        "            inputs = {'input_ids': x['input_ids'], 'attention_mask': x['attention_mask'], 'labels': x['labels']}\n",
        "            if 'token_type_ids' in x:\n",
        "              inputs['token_type_ids'] = x['token_type_ids']\n",
        "            #outputs = model(**inputs)\n",
        "            loss, pred = model(**inputs)\n",
        "\n",
        "            # update parameters\n",
        "            if gradient_accumulation_steps > 1:\n",
        "                loss = loss / gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % gradient_accumulation_steps == 0 or (\n",
        "                # last step in epoch but step is always smaller than gradient_accumulation_steps\n",
        "                len(train_dataloader) <= gradient_accumulation_steps\n",
        "                and (step + 1) == len(train_dataloader)\n",
        "            ):\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                model.zero_grad()\n",
        "                losses.append(tr_loss)\n",
        "                tr_loss = 0.0\n",
        "\n",
        "            # Validation\n",
        "            if (step + 1) == len(train_dataloader):\n",
        "                metricsFullBert = evaluate(model, validation_dataloader, device, tokenizer=tokenizer1)\n",
        "                for name, metric in metricsFullBert.items():\n",
        "                    metrics_hist_fullBert[name].append(metric)\n",
        "\n",
        "            # tensor output\n",
        "            if (step + 1) % eval_steps == 0:\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), np.mean(losses[-10:]), elapsed))\n",
        "\n",
        "            step += 1\n",
        "\n",
        "#evaluation metrics\n",
        "metricsFullBert = evaluate(model, validation_dataloader, device, tokenizer=tokenizer1)\n",
        "for name, metric in metricsFullBert.items():\n",
        "  metrics_hist_fullBert[name].append(metric)\n",
        "\n",
        "#test metrics\n",
        "metrics_hist_FullBert_test = defaultdict(list)\n",
        "metricsFullBert_test = evaluate(model, test_dataloader, device, tokenizer=tokenizer1)\n",
        "for name, metric in metrics_test.items():\n",
        "  metrics_hist_FullBert_test[name].append(metric)\n",
        "\n",
        "#save model\n",
        "out_dir = \"/content/drive/My Drive/Deep Learning Project\"\n",
        "sd = model.state_dict()\n",
        "torch.save(sd, out_dir + \"/model_final_FullBert.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "uGvdzVk7j7Bp",
        "outputId": "09288a4f-f888-41b8-d03f-f58111930997"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1198 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-416fbcab4a39>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m#outputs = model(**inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-952cc1dd59b1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, doc_positions)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mdoc_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     ):\n\u001b[0;32m---> 35\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         )\n\u001b[0;32m-> 1020\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1021\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m                 )\n\u001b[1;32m    609\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    611\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    538\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 0; 39.56 GiB total capacity; 37.39 GiB already allocated; 66.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    }
  ]
}